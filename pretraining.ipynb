{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Tgx4AMZo8anP"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.29.1\n"
          ]
        }
      ],
      "source": [
        "import gymnasium as gym\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "\n",
        "print(f\"{gym.__version__}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "U_1ZNBum8ane"
      },
      "outputs": [],
      "source": [
        "import torch as th\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.nn import functional as F\n",
        "from torch.optim.lr_scheduler import StepLR"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "iaz2Szrl8anx"
      },
      "outputs": [],
      "source": [
        "from stable_baselines3 import PPO, A2C, SAC, TD3\n",
        "from stable_baselines3.common.evaluation import evaluate_policy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "dS9lr70R9eU9"
      },
      "outputs": [],
      "source": [
        "# Example for continuous actions\n",
        "# env_id = \"LunarLanderContinuous-v2\"\n",
        "\n",
        "# Example for discrete actions\n",
        "env_id = \"Pendulum-v1\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "eh88d4oR8an6"
      },
      "outputs": [],
      "source": [
        "env = gym.make(env_id, render_mode=\"human\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rKCgHCc_8aoB"
      },
      "source": [
        "## Train Expert Model\n",
        "\n",
        "We create an expert RL agent and let it learn to solve a task by interacting with the evironment.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "EkmIST0r8aoC"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Wrapping the env with a `Monitor` wrapper\n",
            "Wrapping the env in a DummyVecEnv.\n"
          ]
        }
      ],
      "source": [
        "# ppo_expert = TD3(\"MlpPolicy\", env_id, verbose=1)\n",
        "# ppo_expert.learn(total_timesteps=1e5)\n",
        "# ppo_expert.save(\"ppo_expert\")\n",
        "ppo_expert = TD3.load(\"ppo_expert\", env)\n",
        "# ppo_expert.learn(total_timesteps=1e5)\n",
        "# ppo_expert.save(\"ppo_expert\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hlyTfGAQ_Az1"
      },
      "source": [
        "check the performance of the trained agent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "env = gym.make(env_id)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "-_rVEjC0_AQa"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\ADMIN\\anaconda3\\envs\\carla\\lib\\site-packages\\stable_baselines3\\common\\evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mean reward = -328.06664232872424 +/- 82.09222462959819\n"
          ]
        }
      ],
      "source": [
        "mean_reward, std_reward = evaluate_policy(ppo_expert, env, n_eval_episodes=10)\n",
        "\n",
        "print(f\"Mean reward = {mean_reward} +/- {std_reward}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S-Kv6v_V8aoJ"
      },
      "source": [
        "## Create Student\n",
        "\n",
        "We also create a student RL agent, which will later be trained with the expert dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "fLdLPUeC8aoL"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using cuda device\n",
            "Creating environment from the given name 'Pendulum-v1'\n",
            "Wrapping the env with a `Monitor` wrapper\n",
            "Wrapping the env in a DummyVecEnv.\n"
          ]
        }
      ],
      "source": [
        "student = TD3(\"MlpPolicy\", env_id, verbose=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "sdW8_41-OcXn"
      },
      "outputs": [],
      "source": [
        "# only valid for continuous actions\n",
        "# sac_student = SAC('MlpPolicy', env_id, verbose=1, policy_kwargs=dict(net_arch=[64, 64]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m3GuNxcU8aoT"
      },
      "source": [
        "\n",
        "We now let our expert interact with the environment (except we already have expert data) and store resultant expert observations and actions to build an expert dataset.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "emodyZDW8aoU"
      },
      "outputs": [],
      "source": [
        "num_interactions = int(4e4)\n",
        "# num_interactions = int(510)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "def calculate_returns(rewards, dones, discount_factor=0.99):\n",
        "    returns = []\n",
        "    G = 0  # Initialize the return for the current episode\n",
        "\n",
        "    # Iterate backwards through rewards\n",
        "    for reward, done in zip(reversed(rewards), reversed(dones)):\n",
        "        if done:\n",
        "            G = 0  # Reset the return at the end of each episode\n",
        "        G = reward + discount_factor * G  # Update the return\n",
        "        returns.insert(0, G)  # Insert the return at the beginning of the list\n",
        "\n",
        "    return returns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "F3I_2s808aoZ"
      },
      "outputs": [],
      "source": [
        "# if isinstance(env.action_space, gym.spaces.Box):\n",
        "#     expert_observations = np.empty((num_interactions,) + env.observation_space.shape)\n",
        "#     expert_actions = np.empty((num_interactions,) + (env.action_space.shape[0],))\n",
        "#     expert_returns = np.empty((num_interactions,) + (1,))\n",
        "#     expert_dones = np.empty((num_interactions,) + (1,))\n",
        "\n",
        "# else:\n",
        "#     expert_observations = np.empty((num_interactions,) + env.observation_space.shape)\n",
        "#     expert_actions = np.empty((num_interactions,) + env.action_space.shape)\n",
        "#     expert_returns = np.empty((num_interactions,) + (1,))\n",
        "#     expert_dones = np.empty((num_interactions,) + (1,))\n",
        "\n",
        "# obs, _ = env.reset()\n",
        "\n",
        "# for i in tqdm(range(num_interactions)):\n",
        "#     action, _ = ppo_expert.predict(obs, deterministic=True)\n",
        "#     expert_observations[i] = obs\n",
        "#     expert_actions[i] = action\n",
        "#     obs, reward, terminated, truncated, info = env.step(action)\n",
        "#     expert_returns[i] = reward.astype(np.float32)\n",
        "#     done = terminated or truncated\n",
        "#     expert_dones[i] = done\n",
        "#     if done:\n",
        "#         obs, _ = env.reset()\n",
        "\n",
        "# expert_returns = calculate_returns(expert_returns, expert_dones)\n",
        "# # for idx, done in enumerate(expert_dones):\n",
        "# #     if done:\n",
        "# #         print(idx)\n",
        "# #         break\n",
        "# # print(expert_returns[:idx+3])\n",
        "\n",
        "# np.savez_compressed(\n",
        "#     \"expert_data\",\n",
        "#     expert_actions=expert_actions,\n",
        "#     expert_observations=expert_observations,\n",
        "#     expert_returns=expert_returns,\n",
        "# )\n",
        "\n",
        "# Load the data\n",
        "data = np.load(\"expert_data.npz\")\n",
        "\n",
        "# Access the saved arrays using their keys\n",
        "expert_actions = data['expert_actions']\n",
        "expert_observations = data['expert_observations']\n",
        "expert_returns = data['expert_returns']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "toKEQE9i8aof"
      },
      "source": [
        "\n",
        "\n",
        "- To seamlessly use PyTorch in the training process, we subclass an `ExpertDataset` from PyTorch's base `Dataset`.\n",
        "- Note that we initialize the dataset with the previously generated expert observations and actions.\n",
        "- We further implement Python's `__getitem__` and `__len__` magic functions to allow PyTorch's dataset-handling to access arbitrary rows in the dataset and inform it about the length of the dataset.\n",
        "- For more information about PyTorch's datasets, you can read: https://pytorch.org/docs/stable/data.html.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "qT72bR1i8aog"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data.dataset import Dataset, random_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "xUetr5vl8aom"
      },
      "outputs": [],
      "source": [
        "class ExpertDataSet(Dataset):\n",
        "    def __init__(self, expert_observations, expert_actions, expert_returns):\n",
        "        self.observations = expert_observations\n",
        "        self.actions = expert_actions\n",
        "        self.returns = expert_returns\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        return (self.observations[index], self.actions[index], self.returns[index])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.observations)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K9bNAhXp8aor"
      },
      "source": [
        "\n",
        "\n",
        "We now instantiate the `ExpertDataSet` and split it into training and test datasets.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "lIdT-zMV8aot"
      },
      "outputs": [],
      "source": [
        "expert_dataset = ExpertDataSet(expert_observations, expert_actions, expert_returns)\n",
        "\n",
        "train_size = int(0.8 * len(expert_dataset))\n",
        "\n",
        "test_size = len(expert_dataset) - train_size\n",
        "\n",
        "train_expert_dataset, test_expert_dataset = random_split(\n",
        "    expert_dataset, [train_size, test_size]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "_LgmtFFq8aox"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "test_expert_dataset:  8000\n",
            "train_expert_dataset:  32000\n"
          ]
        }
      ],
      "source": [
        "print(\"test_expert_dataset: \", len(test_expert_dataset))\n",
        "print(\"train_expert_dataset: \", len(train_expert_dataset))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0v8PhG2r8ao4"
      },
      "source": [
        "\n",
        "\n",
        "NOTE: The supervised learning section of this code is adapted from: https://github.com/pytorch/examples/blob/master/mnist/main.py\n",
        "1. We extract the policy network of our RL student agent.\n",
        "2. We load the (labeled) expert dataset containing expert observations as inputs and expert actions as targets.\n",
        "3. We perform supervised learning, that is, we adjust the policy network's parameters such that given expert observations as inputs to the network, its outputs match the targets (expert actions).\n",
        "By training the policy network in this way the corresponding RL student agent is taught to behave like the expert agent that was used to created the expert dataset (Behavior Cloning).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "VwUhCTGU8ao5"
      },
      "outputs": [],
      "source": [
        "def pretrain_agent(\n",
        "    student,\n",
        "    batch_size=64,\n",
        "    epochs=1000,\n",
        "    learning_rate=0.001,\n",
        "    log_interval=100,\n",
        "    no_cuda=False,\n",
        "    seed=1,\n",
        "    test_batch_size=64,\n",
        "):\n",
        "    use_cuda = not no_cuda and th.cuda.is_available()\n",
        "    th.manual_seed(seed)\n",
        "    device = th.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "    kwargs = {\"num_workers\": 1, \"pin_memory\": True} if use_cuda else {}\n",
        "\n",
        "    if isinstance(env.action_space, gym.spaces.Box):\n",
        "        criterion = nn.MSELoss()\n",
        "    else:\n",
        "        criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    # Extract initial policy\n",
        "    actor = student.policy.actor.to(device)\n",
        "    critic = student.policy.critic.to(device)\n",
        "\n",
        "    def train(actor, critic, device, train_loader, actor_optimizer, critic_optimizer):\n",
        "        actor.train()\n",
        "        critic.train()\n",
        "\n",
        "        for batch_idx, (data, target_action, target_return) in enumerate(train_loader):\n",
        "            data, target_action, target_return = data.to(device), target_action.to(device), target_return.to(device)\n",
        "\n",
        "            target_action = target_action.float()\n",
        "            target_return = target_return.float()\n",
        "\n",
        "            action = actor(data)\n",
        "            actor_loss = criterion(action, target_action)\n",
        "            \n",
        "            actor_optimizer.zero_grad()\n",
        "            actor_loss.backward()\n",
        "            actor_optimizer.step()\n",
        "            \n",
        "            current_returns = critic(data, target_action)\n",
        "            critic_loss = sum(F.mse_loss(current_return, target_return) for current_return in current_returns)\n",
        "            \n",
        "            critic_optimizer.zero_grad()\n",
        "            critic_loss.backward()\n",
        "            critic_optimizer.step()\n",
        "\n",
        "            if batch_idx % log_interval == 0:\n",
        "                print(\n",
        "                    \"Train Epoch: {} [{}/{} ({:.0f}%)]\\tActor Loss: {:.6f}\\tCritic Loss: {:.6f}\".format(\n",
        "                        epoch,\n",
        "                        batch_idx * len(data),\n",
        "                        len(train_loader.dataset),\n",
        "                        100.0 * batch_idx / len(train_loader),\n",
        "                        actor_loss.item(),\n",
        "                        critic_loss.item(),\n",
        "                    )\n",
        "                )\n",
        "\n",
        "    def test(actor, critic, device, test_loader):\n",
        "        actor.eval()\n",
        "        critic.eval()\n",
        "        actor_loss = 0\n",
        "        critic_loss = 0\n",
        "        with th.no_grad():\n",
        "            for data, target_action, target_return in test_loader:\n",
        "                data, target_action, target_return = data.to(device), target_action.to(device), target_return.to(device)\n",
        "\n",
        "                target_action = target_action.float()\n",
        "                target_return = target_return.float()\n",
        "                \n",
        "                action = actor(data)\n",
        "                actor_loss += criterion(action, target_action)\n",
        "                current_returns = critic(data, target_action)\n",
        "                critic_loss += sum(F.mse_loss(current_return, target_return) for current_return in current_returns)\n",
        "                \n",
        "        actor_loss /= len(test_loader.dataset)\n",
        "        critic_loss /= len(test_loader.dataset)\n",
        "        print(\n",
        "            \"\\nTest set: Average actor loss: {:.4f}, Average critic loss: {:.4f}\\n\".format(\n",
        "                actor_loss.item(), critic_loss.item()\n",
        "            )\n",
        "        )\n",
        "\n",
        "    # Here, we use PyTorch `DataLoader` to our load previously created `ExpertDataset` for training\n",
        "    # and testing\n",
        "    train_loader = th.utils.data.DataLoader(\n",
        "        dataset=train_expert_dataset, batch_size=batch_size, shuffle=True, **kwargs\n",
        "    )\n",
        "    test_loader = th.utils.data.DataLoader(\n",
        "        dataset=test_expert_dataset,\n",
        "        batch_size=test_batch_size,\n",
        "        shuffle=True,\n",
        "        **kwargs,\n",
        "    )\n",
        "\n",
        "    # Define an Optimizer and a learning rate schedule.\n",
        "    actor_optimizer = optim.Adam(actor.parameters(), lr=learning_rate)\n",
        "    critic_optimizer = optim.Adam(critic.parameters(), lr=learning_rate)\n",
        "\n",
        "    # Now we are finally ready to train the policy model.\n",
        "    for epoch in range(1, epochs + 1):\n",
        "        train(actor, critic, device, train_loader, actor_optimizer, critic_optimizer)\n",
        "        test(actor, critic, device, test_loader)\n",
        "\n",
        "    # Implant the trained policy network back into the RL student agent\n",
        "    student.policy.actor = actor\n",
        "    student.policy.critic = critic"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nkEP6i0hEu_R"
      },
      "source": [
        "Evaluate the agent before pretraining, it should be random"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "_7kvYIneEui8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mean reward = -1414.1589564532042 +/- 235.51497061336397\n"
          ]
        }
      ],
      "source": [
        "mean_reward, std_reward = evaluate_policy(student, env, n_eval_episodes=10)\n",
        "\n",
        "print(f\"Mean reward = {mean_reward} +/- {std_reward}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SgduZAbF8ao9"
      },
      "source": [
        "\n",
        "\n",
        "Having defined the training procedure we can now run the pretraining!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "eI1EFFnW8ao-"
      },
      "outputs": [],
      "source": [
        "pretrain_agent(\n",
        "    student,\n",
        "    epochs=100,\n",
        "    learning_rate=0.001,\n",
        "    log_interval=100,\n",
        "    no_cuda=False,\n",
        "    seed=1,\n",
        "    batch_size=64,\n",
        "    test_batch_size=1000,\n",
        ")\n",
        "student.save(\"student\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RK3Q5Jm58apE"
      },
      "source": [
        "\n",
        "\n",
        "Finally, let us test how well our RL agent student learned to mimic the behavior of the expert\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GKZ8O--m8apF"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mean reward = -147.9170643418096 +/- 69.77307351132359\n"
          ]
        }
      ],
      "source": [
        "mean_reward, std_reward = evaluate_policy(student, env, n_eval_episodes=10)\n",
        "\n",
        "print(f\"Mean reward = {mean_reward} +/- {std_reward}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TwutsYK5Ml9t"
      },
      "outputs": [],
      "source": [
        "student.save(\"td3_expert\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "env = gym.make(env_id, render_mode=\"human\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Wrapping the env with a `Monitor` wrapper\n",
            "Wrapping the env in a DummyVecEnv.\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 200      |\n",
            "|    ep_rew_mean     | -435     |\n",
            "| time/              |          |\n",
            "|    episodes        | 4        |\n",
            "|    fps             | 23       |\n",
            "|    time_elapsed    | 34       |\n",
            "|    total_timesteps | 800      |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 7.43     |\n",
            "|    critic_loss     | 40.8     |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 600      |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 200      |\n",
            "|    ep_rew_mean     | -811     |\n",
            "| time/              |          |\n",
            "|    episodes        | 8        |\n",
            "|    fps             | 22       |\n",
            "|    time_elapsed    | 70       |\n",
            "|    total_timesteps | 1600     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 18.4     |\n",
            "|    critic_loss     | 32.2     |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 1400     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 200      |\n",
            "|    ep_rew_mean     | -943     |\n",
            "| time/              |          |\n",
            "|    episodes        | 12       |\n",
            "|    fps             | 22       |\n",
            "|    time_elapsed    | 106      |\n",
            "|    total_timesteps | 2400     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 26.2     |\n",
            "|    critic_loss     | 32.2     |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 2200     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 200      |\n",
            "|    ep_rew_mean     | -957     |\n",
            "| time/              |          |\n",
            "|    episodes        | 16       |\n",
            "|    fps             | 22       |\n",
            "|    time_elapsed    | 142      |\n",
            "|    total_timesteps | 3200     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 33.2     |\n",
            "|    critic_loss     | 42.9     |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 3000     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 200      |\n",
            "|    ep_rew_mean     | -968     |\n",
            "| time/              |          |\n",
            "|    episodes        | 20       |\n",
            "|    fps             | 22       |\n",
            "|    time_elapsed    | 178      |\n",
            "|    total_timesteps | 4000     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 38.7     |\n",
            "|    critic_loss     | 52.5     |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 3800     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 200      |\n",
            "|    ep_rew_mean     | -927     |\n",
            "| time/              |          |\n",
            "|    episodes        | 24       |\n",
            "|    fps             | 22       |\n",
            "|    time_elapsed    | 214      |\n",
            "|    total_timesteps | 4800     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 42.3     |\n",
            "|    critic_loss     | 65.1     |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 4600     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 200      |\n",
            "|    ep_rew_mean     | -909     |\n",
            "| time/              |          |\n",
            "|    episodes        | 28       |\n",
            "|    fps             | 22       |\n",
            "|    time_elapsed    | 250      |\n",
            "|    total_timesteps | 5600     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 44.5     |\n",
            "|    critic_loss     | 74.1     |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 5400     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 200      |\n",
            "|    ep_rew_mean     | -824     |\n",
            "| time/              |          |\n",
            "|    episodes        | 32       |\n",
            "|    fps             | 22       |\n",
            "|    time_elapsed    | 286      |\n",
            "|    total_timesteps | 6400     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 45.5     |\n",
            "|    critic_loss     | 76       |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 6200     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 200      |\n",
            "|    ep_rew_mean     | -836     |\n",
            "| time/              |          |\n",
            "|    episodes        | 36       |\n",
            "|    fps             | 22       |\n",
            "|    time_elapsed    | 322      |\n",
            "|    total_timesteps | 7200     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 47.7     |\n",
            "|    critic_loss     | 90.6     |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 7000     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 200      |\n",
            "|    ep_rew_mean     | -880     |\n",
            "| time/              |          |\n",
            "|    episodes        | 40       |\n",
            "|    fps             | 22       |\n",
            "|    time_elapsed    | 358      |\n",
            "|    total_timesteps | 8000     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 52.4     |\n",
            "|    critic_loss     | 102      |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 7800     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 200      |\n",
            "|    ep_rew_mean     | -905     |\n",
            "| time/              |          |\n",
            "|    episodes        | 44       |\n",
            "|    fps             | 22       |\n",
            "|    time_elapsed    | 394      |\n",
            "|    total_timesteps | 8800     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 57       |\n",
            "|    critic_loss     | 122      |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 8600     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 200      |\n",
            "|    ep_rew_mean     | -840     |\n",
            "| time/              |          |\n",
            "|    episodes        | 48       |\n",
            "|    fps             | 22       |\n",
            "|    time_elapsed    | 430      |\n",
            "|    total_timesteps | 9600     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 57.5     |\n",
            "|    critic_loss     | 131      |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 9400     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 200      |\n",
            "|    ep_rew_mean     | -795     |\n",
            "| time/              |          |\n",
            "|    episodes        | 52       |\n",
            "|    fps             | 22       |\n",
            "|    time_elapsed    | 466      |\n",
            "|    total_timesteps | 10400    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 55.9     |\n",
            "|    critic_loss     | 123      |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 10200    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 200      |\n",
            "|    ep_rew_mean     | -750     |\n",
            "| time/              |          |\n",
            "|    episodes        | 56       |\n",
            "|    fps             | 22       |\n",
            "|    time_elapsed    | 503      |\n",
            "|    total_timesteps | 11200    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 55.2     |\n",
            "|    critic_loss     | 129      |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 11000    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 200      |\n",
            "|    ep_rew_mean     | -713     |\n",
            "| time/              |          |\n",
            "|    episodes        | 60       |\n",
            "|    fps             | 22       |\n",
            "|    time_elapsed    | 539      |\n",
            "|    total_timesteps | 12000    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 53.3     |\n",
            "|    critic_loss     | 128      |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 11800    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 200      |\n",
            "|    ep_rew_mean     | -679     |\n",
            "| time/              |          |\n",
            "|    episodes        | 64       |\n",
            "|    fps             | 22       |\n",
            "|    time_elapsed    | 575      |\n",
            "|    total_timesteps | 12800    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 52.3     |\n",
            "|    critic_loss     | 126      |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 12600    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 200      |\n",
            "|    ep_rew_mean     | -647     |\n",
            "| time/              |          |\n",
            "|    episodes        | 68       |\n",
            "|    fps             | 22       |\n",
            "|    time_elapsed    | 611      |\n",
            "|    total_timesteps | 13600    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 50.4     |\n",
            "|    critic_loss     | 118      |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 13400    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 200      |\n",
            "|    ep_rew_mean     | -617     |\n",
            "| time/              |          |\n",
            "|    episodes        | 72       |\n",
            "|    fps             | 22       |\n",
            "|    time_elapsed    | 648      |\n",
            "|    total_timesteps | 14400    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 48.9     |\n",
            "|    critic_loss     | 121      |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 14200    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 200      |\n",
            "|    ep_rew_mean     | -594     |\n",
            "| time/              |          |\n",
            "|    episodes        | 76       |\n",
            "|    fps             | 22       |\n",
            "|    time_elapsed    | 684      |\n",
            "|    total_timesteps | 15200    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 47.6     |\n",
            "|    critic_loss     | 130      |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 15000    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 200      |\n",
            "|    ep_rew_mean     | -568     |\n",
            "| time/              |          |\n",
            "|    episodes        | 80       |\n",
            "|    fps             | 22       |\n",
            "|    time_elapsed    | 720      |\n",
            "|    total_timesteps | 16000    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 45.9     |\n",
            "|    critic_loss     | 125      |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 15800    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 200      |\n",
            "|    ep_rew_mean     | -549     |\n",
            "| time/              |          |\n",
            "|    episodes        | 84       |\n",
            "|    fps             | 22       |\n",
            "|    time_elapsed    | 756      |\n",
            "|    total_timesteps | 16800    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 45.7     |\n",
            "|    critic_loss     | 128      |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 16600    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 200      |\n",
            "|    ep_rew_mean     | -533     |\n",
            "| time/              |          |\n",
            "|    episodes        | 88       |\n",
            "|    fps             | 22       |\n",
            "|    time_elapsed    | 793      |\n",
            "|    total_timesteps | 17600    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 43       |\n",
            "|    critic_loss     | 143      |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 17400    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 200      |\n",
            "|    ep_rew_mean     | -521     |\n",
            "| time/              |          |\n",
            "|    episodes        | 92       |\n",
            "|    fps             | 22       |\n",
            "|    time_elapsed    | 829      |\n",
            "|    total_timesteps | 18400    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 42.9     |\n",
            "|    critic_loss     | 121      |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 18200    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 200      |\n",
            "|    ep_rew_mean     | -505     |\n",
            "| time/              |          |\n",
            "|    episodes        | 96       |\n",
            "|    fps             | 22       |\n",
            "|    time_elapsed    | 864      |\n",
            "|    total_timesteps | 19200    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 41.7     |\n",
            "|    critic_loss     | 123      |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 19000    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 200      |\n",
            "|    ep_rew_mean     | -502     |\n",
            "| time/              |          |\n",
            "|    episodes        | 100      |\n",
            "|    fps             | 22       |\n",
            "|    time_elapsed    | 900      |\n",
            "|    total_timesteps | 20000    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 42.5     |\n",
            "|    critic_loss     | 124      |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 19800    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 200      |\n",
            "|    ep_rew_mean     | -491     |\n",
            "| time/              |          |\n",
            "|    episodes        | 104      |\n",
            "|    fps             | 22       |\n",
            "|    time_elapsed    | 936      |\n",
            "|    total_timesteps | 20800    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 40.9     |\n",
            "|    critic_loss     | 124      |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 20600    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 200      |\n",
            "|    ep_rew_mean     | -453     |\n",
            "| time/              |          |\n",
            "|    episodes        | 108      |\n",
            "|    fps             | 22       |\n",
            "|    time_elapsed    | 973      |\n",
            "|    total_timesteps | 21600    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 40.8     |\n",
            "|    critic_loss     | 108      |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 21400    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 200      |\n",
            "|    ep_rew_mean     | -411     |\n",
            "| time/              |          |\n",
            "|    episodes        | 112      |\n",
            "|    fps             | 22       |\n",
            "|    time_elapsed    | 1009     |\n",
            "|    total_timesteps | 22400    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 39.5     |\n",
            "|    critic_loss     | 106      |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 22200    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 200      |\n",
            "|    ep_rew_mean     | -378     |\n",
            "| time/              |          |\n",
            "|    episodes        | 116      |\n",
            "|    fps             | 22       |\n",
            "|    time_elapsed    | 1045     |\n",
            "|    total_timesteps | 23200    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 39.1     |\n",
            "|    critic_loss     | 107      |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 23000    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 200      |\n",
            "|    ep_rew_mean     | -346     |\n",
            "| time/              |          |\n",
            "|    episodes        | 120      |\n",
            "|    fps             | 22       |\n",
            "|    time_elapsed    | 1081     |\n",
            "|    total_timesteps | 24000    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 37.9     |\n",
            "|    critic_loss     | 111      |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 23800    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 200      |\n",
            "|    ep_rew_mean     | -325     |\n",
            "| time/              |          |\n",
            "|    episodes        | 124      |\n",
            "|    fps             | 22       |\n",
            "|    time_elapsed    | 1117     |\n",
            "|    total_timesteps | 24800    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 37.6     |\n",
            "|    critic_loss     | 104      |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 24600    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 200      |\n",
            "|    ep_rew_mean     | -297     |\n",
            "| time/              |          |\n",
            "|    episodes        | 128      |\n",
            "|    fps             | 22       |\n",
            "|    time_elapsed    | 1153     |\n",
            "|    total_timesteps | 25600    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 36.3     |\n",
            "|    critic_loss     | 98.5     |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 25400    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 200      |\n",
            "|    ep_rew_mean     | -297     |\n",
            "| time/              |          |\n",
            "|    episodes        | 132      |\n",
            "|    fps             | 22       |\n",
            "|    time_elapsed    | 1190     |\n",
            "|    total_timesteps | 26400    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 36       |\n",
            "|    critic_loss     | 106      |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 26200    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 200      |\n",
            "|    ep_rew_mean     | -263     |\n",
            "| time/              |          |\n",
            "|    episodes        | 136      |\n",
            "|    fps             | 22       |\n",
            "|    time_elapsed    | 1226     |\n",
            "|    total_timesteps | 27200    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 36       |\n",
            "|    critic_loss     | 97.5     |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 27000    |\n",
            "---------------------------------\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[1;32mc:\\QuangHung\\Autonomous-Driving\\pretraining.ipynb Cell 34\u001b[0m line \u001b[0;36m2\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/QuangHung/Autonomous-Driving/pretraining.ipynb#X44sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m student \u001b[39m=\u001b[39m TD3\u001b[39m.\u001b[39mload(\u001b[39m\"\u001b[39m\u001b[39mtd3_expert\u001b[39m\u001b[39m\"\u001b[39m, env)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/QuangHung/Autonomous-Driving/pretraining.ipynb#X44sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m student\u001b[39m.\u001b[39;49mlearn(total_timesteps\u001b[39m=\u001b[39;49m\u001b[39m1e5\u001b[39;49m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/QuangHung/Autonomous-Driving/pretraining.ipynb#X44sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m student\u001b[39m.\u001b[39msave(\u001b[39m\"\u001b[39m\u001b[39mtd3_expert\u001b[39m\u001b[39m\"\u001b[39m)\n",
            "File \u001b[1;32mc:\\Users\\ADMIN\\anaconda3\\envs\\carla\\lib\\site-packages\\stable_baselines3\\td3\\td3.py:222\u001b[0m, in \u001b[0;36mTD3.learn\u001b[1;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[0;32m    213\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mlearn\u001b[39m(\n\u001b[0;32m    214\u001b[0m     \u001b[39mself\u001b[39m: SelfTD3,\n\u001b[0;32m    215\u001b[0m     total_timesteps: \u001b[39mint\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    220\u001b[0m     progress_bar: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[0;32m    221\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m SelfTD3:\n\u001b[1;32m--> 222\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mlearn(\n\u001b[0;32m    223\u001b[0m         total_timesteps\u001b[39m=\u001b[39;49mtotal_timesteps,\n\u001b[0;32m    224\u001b[0m         callback\u001b[39m=\u001b[39;49mcallback,\n\u001b[0;32m    225\u001b[0m         log_interval\u001b[39m=\u001b[39;49mlog_interval,\n\u001b[0;32m    226\u001b[0m         tb_log_name\u001b[39m=\u001b[39;49mtb_log_name,\n\u001b[0;32m    227\u001b[0m         reset_num_timesteps\u001b[39m=\u001b[39;49mreset_num_timesteps,\n\u001b[0;32m    228\u001b[0m         progress_bar\u001b[39m=\u001b[39;49mprogress_bar,\n\u001b[0;32m    229\u001b[0m     )\n",
            "File \u001b[1;32mc:\\Users\\ADMIN\\anaconda3\\envs\\carla\\lib\\site-packages\\stable_baselines3\\common\\off_policy_algorithm.py:333\u001b[0m, in \u001b[0;36mOffPolicyAlgorithm.learn\u001b[1;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[0;32m    331\u001b[0m         \u001b[39m# Special case when the user passes `gradient_steps=0`\u001b[39;00m\n\u001b[0;32m    332\u001b[0m         \u001b[39mif\u001b[39;00m gradient_steps \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m--> 333\u001b[0m             \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain(batch_size\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbatch_size, gradient_steps\u001b[39m=\u001b[39;49mgradient_steps)\n\u001b[0;32m    335\u001b[0m callback\u001b[39m.\u001b[39mon_training_end()\n\u001b[0;32m    337\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n",
            "File \u001b[1;32mc:\\Users\\ADMIN\\anaconda3\\envs\\carla\\lib\\site-packages\\stable_baselines3\\td3\\td3.py:179\u001b[0m, in \u001b[0;36mTD3.train\u001b[1;34m(self, gradient_steps, batch_size)\u001b[0m\n\u001b[0;32m    176\u001b[0m     target_q_values \u001b[39m=\u001b[39m replay_data\u001b[39m.\u001b[39mrewards \u001b[39m+\u001b[39m (\u001b[39m1\u001b[39m \u001b[39m-\u001b[39m replay_data\u001b[39m.\u001b[39mdones) \u001b[39m*\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgamma \u001b[39m*\u001b[39m next_q_values\n\u001b[0;32m    178\u001b[0m \u001b[39m# Get current Q-values estimates for each critic network\u001b[39;00m\n\u001b[1;32m--> 179\u001b[0m current_q_values \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcritic(replay_data\u001b[39m.\u001b[39;49mobservations, replay_data\u001b[39m.\u001b[39;49mactions)\n\u001b[0;32m    181\u001b[0m \u001b[39m# Compute critic loss\u001b[39;00m\n\u001b[0;32m    182\u001b[0m critic_loss \u001b[39m=\u001b[39m \u001b[39msum\u001b[39m(F\u001b[39m.\u001b[39mmse_loss(current_q, target_q_values) \u001b[39mfor\u001b[39;00m current_q \u001b[39min\u001b[39;00m current_q_values)\n",
            "File \u001b[1;32mc:\\Users\\ADMIN\\anaconda3\\envs\\carla\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
            "File \u001b[1;32mc:\\Users\\ADMIN\\anaconda3\\envs\\carla\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\ADMIN\\anaconda3\\envs\\carla\\lib\\site-packages\\stable_baselines3\\common\\policies.py:937\u001b[0m, in \u001b[0;36mContinuousCritic.forward\u001b[1;34m(self, obs, actions)\u001b[0m\n\u001b[0;32m    935\u001b[0m     features \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mextract_features(obs, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfeatures_extractor)\n\u001b[0;32m    936\u001b[0m qvalue_input \u001b[39m=\u001b[39m th\u001b[39m.\u001b[39mcat([features, actions], dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m--> 937\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mtuple\u001b[39;49m(q_net(qvalue_input) \u001b[39mfor\u001b[39;49;00m q_net \u001b[39min\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mq_networks)\n",
            "File \u001b[1;32mc:\\Users\\ADMIN\\anaconda3\\envs\\carla\\lib\\site-packages\\stable_baselines3\\common\\policies.py:937\u001b[0m, in \u001b[0;36m<genexpr>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    935\u001b[0m     features \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mextract_features(obs, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfeatures_extractor)\n\u001b[0;32m    936\u001b[0m qvalue_input \u001b[39m=\u001b[39m th\u001b[39m.\u001b[39mcat([features, actions], dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m--> 937\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mtuple\u001b[39m(q_net(qvalue_input) \u001b[39mfor\u001b[39;00m q_net \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mq_networks)\n",
            "File \u001b[1;32mc:\\Users\\ADMIN\\anaconda3\\envs\\carla\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
            "File \u001b[1;32mc:\\Users\\ADMIN\\anaconda3\\envs\\carla\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\ADMIN\\anaconda3\\envs\\carla\\lib\\site-packages\\torch\\nn\\modules\\container.py:215\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    213\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[0;32m    214\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[1;32m--> 215\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[0;32m    216\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
            "File \u001b[1;32mc:\\Users\\ADMIN\\anaconda3\\envs\\carla\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
            "File \u001b[1;32mc:\\Users\\ADMIN\\anaconda3\\envs\\carla\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\ADMIN\\anaconda3\\envs\\carla\\lib\\site-packages\\torch\\nn\\modules\\linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m--> 114\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "student = TD3.load(\"td3_expert\", env)\n",
        "student.learn(total_timesteps=1e5)\n",
        "student.save(\"td3_expert\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "pretraining.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.18"
    },
    "vscode": {
      "interpreter": {
        "hash": "3201c96db5836b171d01fee72ea1be894646622d4b41771abf25c98b548a611d"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
